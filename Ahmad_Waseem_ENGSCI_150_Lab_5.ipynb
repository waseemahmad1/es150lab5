{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waseemahmad1/es150lab5/blob/main/Ahmad_Waseem_ENGSCI_150_Lab_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ES150: Introduction to Probability with Engineering Applications**\n",
        "\n",
        "Lab #5: Markov Chains and Applications\n",
        "\n",
        "Spring 2024"
      ],
      "metadata": {
        "id": "aHQiFAy8-A7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, we will use simulations to verify the theoretical characterizations of the steady-state distributions of Markov chains. We will also explore an application to Markov chains in natural lanaguge modelling."
      ],
      "metadata": {
        "id": "tdTZwGGe-MZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part I: Steady-State Distributions of Markov chains**\n",
        "\n",
        "Consider a 3-state Markov chain, whose transition matrix is given by\n",
        "\n",
        "\\begin{align}\n",
        "P = \\begin{bmatrix}0.2 & 0.4 & 0.4\\\\\n",
        "0.45 & 0.45 & 0.1\\\\\n",
        "0.3 & 0.3  & 0.4 \\end{bmatrix}.\n",
        "\\end{align}\n",
        "\n",
        "In the lectures, we have seen the following characterization of the long-term \"steady-state\" behaviors of this Markov chain:\n",
        "\n",
        "\\begin{align}\\tag{1}\n",
        "\\lim_{n \\to \\infty} P^n = \\begin{bmatrix}\\pi_1 & \\pi_2 & \\pi_3\\\\\n",
        "\\pi_1 & \\pi_2 & \\pi_3\\\\\n",
        "\\pi_1 & \\pi_2  & \\pi_3 \\end{bmatrix}.\n",
        "\\end{align}\n",
        "\n",
        "In the above formula, $\\pi_1, \\pi_2, \\pi_3$ are three numbers that can be determined by the following conditions:\n",
        "\n",
        "1.   Normalization:\n",
        "\\begin{align}\n",
        "\\pi_1 + \\pi_2 + \\pi_3 = 1.\n",
        "\\end{align}\n",
        "2.   Balance equation:\n",
        "\\begin{align}\n",
        "\\begin{bmatrix}\\pi_1 & \\pi_2 & \\pi_3\\end{bmatrix} = \\begin{bmatrix}\\pi_1 & \\pi_2 & \\pi_3\\end{bmatrix} P.\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "inXmNloA-idS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 1: Create a numpy array for P***"
      ],
      "metadata": {
        "id": "HnEZdZMqClMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# type your code here: you can use np.array\n",
        "\n",
        "P = np.array([\n",
        "    [0.2, 0.4, 0.4],\n",
        "    [0.45, 0.45, 0.1],\n",
        "    [0.3, 0.3, 0.4]])\n",
        "\n",
        "print(P)"
      ],
      "metadata": {
        "id": "1_RMwKQSCzAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b5c768-3c16-4ae1-d63c-20747b4204b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.2  0.4  0.4 ]\n",
            " [0.45 0.45 0.1 ]\n",
            " [0.3  0.3  0.4 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 2: Take several different integer powers of P***\n",
        "\n",
        "Compute $P^2, P^5, P^{10}$ and $P^{100}$. Print these matrices, and verify that, as $n$ increases, $P^n$ indeed converges to the form $\\begin{bmatrix}\\pi_1 & \\pi_2 & \\pi_3\\\\\n",
        "\\pi_1 & \\pi_2 & \\pi_3\\\\\n",
        "\\pi_1 & \\pi_2  & \\pi_3 \\end{bmatrix}$.\n",
        "\n",
        "Moreover, verify that $\\pi_1, \\pi_2, \\pi_3$ that you get from $P^{100}$ indeed satisfy the normalization and balence equations.\n",
        "\n"
      ],
      "metadata": {
        "id": "Kqv8OzgKDLp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type your code here\n",
        "# Hint: To compute matrix power, you should use np.linalg.matrix_power(P, n), where n is the power\n",
        "\n",
        "P = np.array([\n",
        "    [0.2, 0.4, 0.4],\n",
        "    [0.45, 0.45, 0.1],\n",
        "    [0.3, 0.3, 0.4]])\n",
        "\n",
        "second_power = np.linalg.matrix_power(P, 2)\n",
        "fifth_power = np.linalg.matrix_power(P, 5)\n",
        "tenth_power = np.linalg.matrix_power(P, 10)\n",
        "one_hundreth_power = np.linalg.matrix_power(P, 100)\n",
        "one_thousandth_power = np.linalg.matrix_power(P, 1000)\n",
        "\n",
        "print(second_power, \"\\n\")\n",
        "print(fifth_power, \"\\n\")\n",
        "print(tenth_power, \"\\n\")\n",
        "print(one_hundreth_power, \"\\n\")\n",
        "print(one_thousandth_power, \"\\n\")\n",
        "\n",
        "row_sums_one_hundreth = np.sum(one_hundreth_power, axis=1)\n",
        "product = np.dot(P, one_hundreth_power)\n",
        "\n",
        "print(\"Normalization (pi_1 + pi_2 + pi_3): \", row_sums_one_hundreth)\n",
        "\n",
        "print(\"Balance Equation:\\n\", product, \"\\n\")\n",
        "\n",
        "\n",
        "print(\"Hence, we have indeed satisfied the normalization and balance equations\")"
      ],
      "metadata": {
        "id": "pp-6M72lDDNe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cea0636-64e1-4129-d541-0570ecdc63ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.34   0.38   0.28  ]\n",
            " [0.3225 0.4125 0.265 ]\n",
            " [0.315  0.375  0.31  ]] \n",
            "\n",
            "[[0.3260075  0.3912775  0.282715  ]\n",
            " [0.32619656 0.39142781 0.28237563]\n",
            " [0.32602688 0.39116438 0.28280875]] \n",
            "\n",
            "[[0.32608695 0.39130433 0.28260871]\n",
            " [0.32608698 0.39130439 0.28260863]\n",
            " [0.32608693 0.3913043  0.28260876]] \n",
            "\n",
            "[[0.32608696 0.39130435 0.2826087 ]\n",
            " [0.32608696 0.39130435 0.2826087 ]\n",
            " [0.32608696 0.39130435 0.2826087 ]] \n",
            "\n",
            "[[0.32608696 0.39130435 0.2826087 ]\n",
            " [0.32608696 0.39130435 0.2826087 ]\n",
            " [0.32608696 0.39130435 0.2826087 ]] \n",
            "\n",
            "Normalization (pi_1 + pi_2 + pi_3):  [1. 1. 1.]\n",
            "Balance Equation:\n",
            " [[0.32608696 0.39130435 0.2826087 ]\n",
            " [0.32608696 0.39130435 0.2826087 ]\n",
            " [0.32608696 0.39130435 0.2826087 ]] \n",
            "\n",
            "Hence, we have indeed satisfied the normalization and balance equations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 3: The long-term frequency of visiting a given state***\n",
        "\n",
        "In the lectures, we provide one interpretation of the probabilities $\\pi_1, \\pi_3, \\pi_3$. Suppose we simulate the Markov chain for $n$ steps. Let $V_1(n)$ denote the number of times the state 1 has been visited. Similarly, we can define $V_2(n)$ and $V_3(n)$ to count the number of times states 2 and 3 have been visited within $n$ steps of the Markov chain transition. Then we know that\n",
        "\\begin{align}\n",
        "  \\lim_{n \\to \\infty} \\frac{V_1(n)}{n} = \\pi_1, \\qquad  \\lim_{n \\to \\infty} \\frac{V_2(n)}{n} = \\pi_2, \\qquad  \\lim_{n \\to \\infty} \\frac{V_3(n)}{n} = \\pi_3.\n",
        "\\end{align}\n",
        "\n",
        "In this step, we will verify the above characterization by simulating the Markov chain for $n = 10^5$ steps."
      ],
      "metadata": {
        "id": "2u8EV2DqGjLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type your code here\n",
        "\n",
        "n = 10**5 # length of simulations\n",
        "\n",
        "visits = np.zeros(3) # an array recording the number of times each of the three states have been visited\n",
        "\n",
        "curr_state = 0 # suppose we start the Markov chain at state 1. Note that we encode state 1 by index 0, due to NumPy's 0-based indexing scheme.\n",
        "\n",
        "# continue your code here. To simulate the evolution of Markov chains, you can use np.random.choice\n",
        "\n",
        "for _ in range(n):\n",
        "    curr_state = np.random.choice([0, 1, 2], p=P[curr_state])\n",
        "    visits[curr_state] += 1\n",
        "\n",
        "probability = visits / n\n",
        "\n",
        "print(\"Empirical probabilities:\", probability, \"\\n\")\n",
        "print(one_hundreth_power, \"\\n\")\n",
        "\n",
        "print(\"We can see that our probabilities are close to pi_1, pi_2, and pi_3. If n was even larger, we should see these probabilities become very very very close/equivalent\")"
      ],
      "metadata": {
        "id": "MTlAOV5xH1E6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f950f09e-02d3-45b6-9872-050a356d343f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empirical probabilities: [0.32481 0.39056 0.28463] \n",
            "\n",
            "[[0.32608696 0.39130435 0.2826087 ]\n",
            " [0.32608696 0.39130435 0.2826087 ]\n",
            " [0.32608696 0.39130435 0.2826087 ]] \n",
            "\n",
            "We can see that our probabilities are close to pi_1, pi_2, and pi_3. If n was even larger, we should see these probabilities become very very very close/equivalent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part II: Application of Markov Chains: Modelling and generating English sentences**\n",
        "\n",
        "In this part, we will delve into a significant application of Markov chains—the modeling of natural languages. Our approach involves using a text file, such as a classical novel, as the input. Each unique word and punctuation mark within the text is mapped to a unique state (in a process known as tokenization).\n",
        "\n",
        "For the purposes of our model, suppose the text contains 3000 unique words and punctuation marks. This setup leads us to construct a Markov chain consisting of 3000 states. The transitions between these states are determined by a $3000 \\times 3000$ transition matrix $P$. We will learn this matrix directly from the input text to accurately reflect the linguistic structure of the source material.\n",
        "\n",
        "Once the Markov chain is established and the transition matrix is learned, we can use this model to generate new sentences. These sentences are expected to resemble coherent English language constructs, mirroring the style and vocabulary of the input text."
      ],
      "metadata": {
        "id": "orav3TgQXwVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Read the input text and map words/punctuation marks into unique tokens**"
      ],
      "metadata": {
        "id": "vbFaYsE-Z2Be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following are some utilitie functions\n",
        "\n",
        "def read_file(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "import re, nltk\n",
        "\n",
        "def preprocess(text):\n",
        "    # Ensure space around punctuation\n",
        "    text = re.sub(r'([.,;:!?\\(\\)])', r' \\1 ', text)\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "nltk.download('punkt')  # Download the necessary datasets\n",
        "\n",
        "def tokenize(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return words\n",
        "\n",
        "def map_words_to_tokens(words):\n",
        "    word_to_index = {}\n",
        "    index_to_word = {}\n",
        "    for word in words:\n",
        "        if word not in word_to_index:\n",
        "            index = len(word_to_index)\n",
        "            word_to_index[word] = index\n",
        "            index_to_word[index] = word\n",
        "    return word_to_index, index_to_word\n",
        "\n",
        "from urllib.request import urlopen\n",
        "\n",
        "def read_text_from_url_builtin(url):\n",
        "    with urlopen(url) as response:\n",
        "        text = response.read().decode('utf-8')\n",
        "    return text\n",
        "\n",
        "import textwrap\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wWEeUyOaA5E",
        "outputId": "780f80c5-91e5-4d8e-9474-0d080de576a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's specify the source material. Here I use Alice's Adventures in Wonderland. You should experiment with different sources\n",
        "url1 = 'https://www.gutenberg.org/cache/epub/11/pg11.txt'\n",
        "\n",
        "text1 = read_text_from_url_builtin(url)\n",
        "text1 = preprocess(text)\n",
        "\n",
        "words1 = tokenize(text) # words is an array containing consecutive words in the text. You can print it to examine its content.\n",
        "\n",
        "# These two mappings allow us to map unique words to integers (i.e. state of a Markov chain) and vice versa.\n",
        "word_to_index, index_to_word = map_words_to_tokens(words1)"
      ],
      "metadata": {
        "id": "zujhZW10hAua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Type your code here\n",
        "\n",
        "# 1. Print words, to examine its context\n",
        "\n",
        "print(words1)\n",
        "\n",
        "# 2. Print index_to_word[10], index_to_word[100], word_to_index['This'] ... in order to understand what these two dictionaries are really doing.\n",
        "\n",
        "print(index_to_word[10])\n",
        "print(index_to_word[100])\n",
        "print(word_to_index['This'])\n",
        "\n",
        "# 3. To test your understanding, what is the index associated with the word \"mouse\"?\n",
        "\n",
        "print(\"The index associated with the word 'mouse' is:\", word_to_index['mouse'])"
      ],
      "metadata": {
        "id": "eM2KkeIwiteE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gatsby_url2 = 'https://www.gutenberg.org/cache/epub/64317/pg64317.txt'\n",
        "\n",
        "text2 = read_text_from_url_builtin(gatsby_url2)\n",
        "text2 = preprocess(text2)\n",
        "words2 = tokenize(text2)\n",
        "word_to_index, index_to_word = map_words_to_tokens(words2)\n",
        "\n",
        "print(words2)\n",
        "print(index_to_word[1000])\n",
        "print(index_to_word[1500])\n",
        "print(word_to_index['This'])\n",
        "print(\"The index associated with the word 'Daisy' is:\", word_to_index['Daisy'])"
      ],
      "metadata": {
        "id": "pGHFOK9BWC_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Learn the transition matrix of the Markov chain**"
      ],
      "metadata": {
        "id": "_tCALlV8w-EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type your code here\n",
        "\n",
        "M = len(word_to_index) # M is the number of states in the Markov chain\n",
        "P = np.zeros((M, M))\n",
        "\n",
        "# Learn the transition matrix P from the input text\n",
        "# Don't forget to normalize P so that it's a valid transition matrix\n",
        "\n",
        "transition_counts = np.zeros((M, M))\n",
        "\n",
        "for i in range(len(words1) - 1):\n",
        "    current_word_index = word_to_index[words1[i]]\n",
        "    next_word_index = word_to_index[words1[i+1]]\n",
        "    transition_counts[current_word_index, next_word_index] += 1\n",
        "\n",
        "# Normalize\n",
        "P = transition_counts / transition_counts.sum(axis=1, keepdims=True)\n",
        "\n",
        "print(P)"
      ],
      "metadata": {
        "id": "CibcghTXi6J7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "195236ea-b3ea-4128-b92b-97e03f23e4a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         1.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.33333333 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         1.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Generate new sentences from the learned Markov chain**"
      ],
      "metadata": {
        "id": "k64C8yqOxbWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type your code here\n",
        "\n",
        "nsamples = 5 # how many different sentences we want to generate\n",
        "sentence_len = 100 # length of each sentence\n",
        "\n",
        "sentences = []\n",
        "\n",
        "for _ in range(nsamples):\n",
        "    current_state = np.random.choice(M)\n",
        "    sentence = [index_to_word[current_state]]\n",
        "\n",
        "    for _ in range(sentence_len - 1):\n",
        "        current_state = np.random.choice(M, p=P[current_state])\n",
        "        sentence.append(index_to_word[current_state])\n",
        "\n",
        "    sentence_str = ' '.join(sentence)\n",
        "    sentences.append(sentence_str)\n",
        "\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"{sentence}\\n\")"
      ],
      "metadata": {
        "id": "ijxRho2HjCgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21242e59-1a6e-4e9b-cf5a-de09f435db76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrupt data , and then at all else . “ Now , as follows : — “ You may be listening , ” he spoke at once or 1 . “ Off with the sea— ” “ until there seemed to it . “ Sure , and Northumbria , ” ( trademark/copyright ) “ she went off , who was walking hand upon Bill , ” the trees under its head unless you if you for fear lest she had happened , going out with its waistcoat-pocket_ , “ I give all the terms of sitting by U . “\n",
            "\n",
            "shake at everything that it , “ Can you were never forgotten that , ” the Tarts ? ” “ I can do such a little of her idea what porpoise , but I wonder what it , ” said : she went on their own business , they used up at first question the Queen ’ s done , and then a wonderful Adventures in a moment : “ What ! wow ! ” said the method you in her . ” “ Go on tiptoe , “ Suppose we can _ever_ finish , “ How fond of eating\n",
            "\n",
            "custard , swallowing down , ” but then—I shouldn ’ ll stay down at the Dormouse crossed the dance . “ if I want to abide by two guinea-pigs , and addresses . “ on one elbow was quite dry leaves , ” it said Alice to say to the night-air doesn ’ d been changed , and licensed works in the Mock Turtle . ” said the matter which was much like a mile high , ” she had found at processions ; so much sooner or courtiers , and passed by Lewis Carroll THE END OF HEARTS .\n",
            "\n",
            "prison , and he consented to try and very little girls eat a large round ! the air of long claws and I shall ever getting quite impossible to remark . All this time . _I_ shan ’ t know what Latitude or later editions will you liked teaching it didn ’ tis love , I can do not met in fact , for copies of _that_ like mad here poor Alice . ” said the King , she quite unhappy at Alice said the White Rabbit , old , but she was , my life ! ” the chimney\n",
            "\n",
            "hollow tone , and then after this time , she hastily . “ I wonder is which she kept her for them . I _beg_ your evidence , and Alice , ” On various pretexts they take the roof of the sun , ancient and she appeared , Serpent ! Beau—ootiful Soo—oop ! ” said Alice cautiously : the phrase “ I ’ t help it didn ’ t know whether it left no such as follows The Knave . ” The invalidity or fig ? ” she stopped hastily . 1 . Alice . “ nine feet , “\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Optional) Step 4: Invitation to improve the code and the modeling**\n",
        "\n",
        "As we continue to refine our code and model for generating text, I want to outline some potential improvements that could significantly increase the quality of the sentences we produce. By implementing these strategies, we can better mimic the natural flow of English language. This (optional) part is meant as an open-ended invitation to all of you to experiment with new ideas. I look forward to seeing how far we can push the boundaries.\n",
        "\n",
        "1. Second-Order Markov Chain: This approach involves extending our current model to consider the two most recent words (a \"context window\" of two) instead of just the last one. By accounting for both the current and the previous word, this method could more accurately capture the complex statistical correlations typically found in English sentence structures.\n",
        "\n",
        "2. Start and End Symbols: Another enhancement could involve integrating special symbols that specifically denote the start and end of sentences. This adjustment would help in generating more coherent and contextually appropriate sentences, providing clear cues about sentence boundaries which are essential for realistic text generation.\n",
        "\n",
        "3. Consider using more advanced models such as recurrent neural networks (RNNs), especially LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Units), which are capable of remembering longer sequences of data and can generate more contextually relevant sentences.\n"
      ],
      "metadata": {
        "id": "_Yv_FEpny7zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Type your code here\n"
      ],
      "metadata": {
        "id": "QGTTkmLTkau0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}